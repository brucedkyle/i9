{"cells":[{"cell_type":"markdown","id":"5c2f2621-074e-45a3-b892-9fd4ac5dbc2a","metadata":{"language":"python","id":"5c2f2621-074e-45a3-b892-9fd4ac5dbc2a"},"source":["# Agentic RAG Using CrewAI & LangChain\n","\n","We are going to see how agents can be involved in the RAG system to rtrieve the most relevant information."]},{"cell_type":"code","execution_count":13,"id":"2408f415-d51c-4d75-ad71-e809b7072778","metadata":{"execution":{"iopub.execute_input":"2024-08-06T08:52:51.355586Z","iopub.status.busy":"2024-08-06T08:52:51.355253Z","iopub.status.idle":"2024-08-06T08:53:07.107101Z","shell.execute_reply":"2024-08-06T08:53:07.105613Z","shell.execute_reply.started":"2024-08-06T08:52:51.355560Z"},"language":"python","trusted":true,"id":"2408f415-d51c-4d75-ad71-e809b7072778","executionInfo":{"status":"ok","timestamp":1736957301536,"user_tz":480,"elapsed":5467,"user":{"displayName":"Bruce Kyle","userId":"12761463034575353107"}}},"outputs":[],"source":["!pip install crewai==0.28.8 crewai_tools==0.1.6 langchain_community==0.0.29 sentence-transformers langchain-groq langchain_huggingface --quiet"]},{"cell_type":"code","execution_count":14,"id":"7d74a183-c473-4286-aa91-6ac1c89f8a08","metadata":{"execution":{"iopub.execute_input":"2024-08-06T08:53:11.142412Z","iopub.status.busy":"2024-08-06T08:53:11.141324Z","iopub.status.idle":"2024-08-06T08:53:17.696025Z","shell.execute_reply":"2024-08-06T08:53:17.695307Z","shell.execute_reply.started":"2024-08-06T08:53:11.142373Z"},"language":"python","trusted":true,"id":"7d74a183-c473-4286-aa91-6ac1c89f8a08","executionInfo":{"status":"ok","timestamp":1736957301537,"user_tz":480,"elapsed":5,"user":{"displayName":"Bruce Kyle","userId":"12761463034575353107"}}},"outputs":[],"source":["from langchain_openai import ChatOpenAI\n","import os\n","from crewai_tools import PDFSearchTool\n","from langchain_community.tools.tavily_search import TavilySearchResults\n","from crewai_tools  import tool\n","from crewai import Crew\n","from crewai import Task\n","from crewai import Agent"]},{"cell_type":"markdown","id":"f537530b-c530-43d6-afb7-d6a24d237908","metadata":{"language":"python","id":"f537530b-c530-43d6-afb7-d6a24d237908"},"source":["## Mention the Groq API Key"]},{"cell_type":"code","execution_count":15,"id":"b211674e-aed8-4732-beef-7c3922279138","metadata":{"execution":{"iopub.execute_input":"2024-08-06T08:53:26.673091Z","iopub.status.busy":"2024-08-06T08:53:26.672457Z","iopub.status.idle":"2024-08-06T08:53:26.678191Z","shell.execute_reply":"2024-08-06T08:53:26.677631Z","shell.execute_reply.started":"2024-08-06T08:53:26.673058Z"},"language":"python","trusted":true,"id":"b211674e-aed8-4732-beef-7c3922279138","executionInfo":{"status":"ok","timestamp":1736957302137,"user_tz":480,"elapsed":604,"user":{"displayName":"Bruce Kyle","userId":"12761463034575353107"}}},"outputs":[],"source":["import os\n","from google.colab import userdata\n","GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n","\n","# Set the API key\n","os.environ['GROQ_API_KEY'] = GROQ_API_KEY"]},{"cell_type":"markdown","id":"61808e55-3a5c-4977-a36f-6487740d20c9","metadata":{"language":"python","id":"61808e55-3a5c-4977-a36f-6487740d20c9"},"source":["## Mention the LLM being used"]},{"cell_type":"code","execution_count":16,"id":"dcb3e7b8-04c1-4a4b-a78f-9f10cef13222","metadata":{"execution":{"iopub.execute_input":"2024-08-06T08:54:42.224934Z","iopub.status.busy":"2024-08-06T08:54:42.224560Z","iopub.status.idle":"2024-08-06T08:54:42.268360Z","shell.execute_reply":"2024-08-06T08:54:42.267668Z","shell.execute_reply.started":"2024-08-06T08:54:42.224905Z"},"language":"python","trusted":true,"id":"dcb3e7b8-04c1-4a4b-a78f-9f10cef13222","executionInfo":{"status":"ok","timestamp":1736957302344,"user_tz":480,"elapsed":209,"user":{"displayName":"Bruce Kyle","userId":"12761463034575353107"}}},"outputs":[],"source":["llm = ChatOpenAI(\n","    openai_api_base=\"https://api.groq.com/openai/v1\",\n","    openai_api_key=os.environ['GROQ_API_KEY'],\n","    model_name=\"llama3-8b-8192\",\n","    temperature=0.1,\n","    max_tokens=1000,\n",")"]},{"cell_type":"code","execution_count":17,"id":"be093f01-bb3d-483e-9ea2-0ca9c5875000","metadata":{"execution":{"iopub.execute_input":"2024-08-06T09:09:42.443927Z","iopub.status.busy":"2024-08-06T09:09:42.443522Z","iopub.status.idle":"2024-08-06T09:09:43.219922Z","shell.execute_reply":"2024-08-06T09:09:43.219238Z","shell.execute_reply.started":"2024-08-06T09:09:42.443896Z"},"language":"python","trusted":true,"id":"be093f01-bb3d-483e-9ea2-0ca9c5875000","executionInfo":{"status":"ok","timestamp":1736957303029,"user_tz":480,"elapsed":687,"user":{"displayName":"Bruce Kyle","userId":"12761463034575353107"}}},"outputs":[],"source":["import requests\n","\n","pdf_url = 'https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf'\n","response = requests.get(pdf_url)\n","\n","with open('attenstion_is_all_you_need.pdf', 'wb') as file:\n","    file.write(response.content)"]},{"cell_type":"markdown","id":"c2cfdc74-f167-4a14-a4d6-6094cb5b71a0","metadata":{"language":"python","id":"c2cfdc74-f167-4a14-a4d6-6094cb5b71a0"},"source":["## Create a RAG tool variable to pass our PDF"]},{"cell_type":"code","execution_count":18,"id":"d97d2f22-8667-4799-bf90-1a9fec1dc540","metadata":{"execution":{"iopub.execute_input":"2024-08-06T09:10:08.404340Z","iopub.status.busy":"2024-08-06T09:10:08.403918Z","iopub.status.idle":"2024-08-06T09:10:12.883348Z","shell.execute_reply":"2024-08-06T09:10:12.880732Z","shell.execute_reply.started":"2024-08-06T09:10:08.404297Z"},"language":"python","trusted":true,"id":"d97d2f22-8667-4799-bf90-1a9fec1dc540","executionInfo":{"status":"ok","timestamp":1736957307634,"user_tz":480,"elapsed":4607,"user":{"displayName":"Bruce Kyle","userId":"12761463034575353107"}}},"outputs":[],"source":["rag_tool = PDFSearchTool(pdf='attenstion_is_all_you_need.pdf',\n","    config=dict(\n","        llm=dict(\n","            provider=\"groq\", # or google, openai, anthropic, llama2, ...\n","            config=dict(\n","                model=\"llama3-8b-8192\",\n","                # temperature=0.5,\n","                # top_p=1,\n","                # stream=true,\n","            ),\n","        ),\n","        embedder=dict(\n","            provider=\"huggingface\", # or openai, ollama, ...\n","            config=dict(\n","                model=\"BAAI/bge-small-en-v1.5\",\n","                #task_type=\"retrieval_document\",\n","                # title=\"Embeddings\",\n","            ),\n","        ),\n","    )\n",")"]},{"cell_type":"code","execution_count":19,"id":"40e49ecd-7cd8-464c-be2f-5e4aa1f58de6","metadata":{"execution":{"iopub.execute_input":"2024-08-06T09:10:29.400488Z","iopub.status.busy":"2024-08-06T09:10:29.399575Z","iopub.status.idle":"2024-08-06T09:10:29.479174Z","shell.execute_reply":"2024-08-06T09:10:29.475340Z","shell.execute_reply.started":"2024-08-06T09:10:29.400441Z"},"language":"python","trusted":true,"id":"40e49ecd-7cd8-464c-be2f-5e4aa1f58de6","outputId":"26223375-7d89-417f-a5ab-07b7ef7a28ec","colab":{"base_uri":"https://localhost:8080/","height":177},"executionInfo":{"status":"ok","timestamp":1736957307878,"user_tz":480,"elapsed":246,"user":{"displayName":"Bruce Kyle","userId":"12761463034575353107"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Using Tool: Search a PDF's content\n"]},{"output_type":"execute_result","data":{"text/plain":["'Relevant Content:\\nThis makes it more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2. Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 22, 23, 19]. End-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [28]. To the best of our knowledge, however, the Transformer\\n\\nIn terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence lengthnis smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [31] and byte-pair [ 25] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size rin 6\\n\\nAttention Is All You Need Ashish Vaswani∗ Google Brain avaswani@google.comNoam Shazeer∗ Google Brain noam@google.comNiki Parmar∗ Google Research nikip@google.comJakob Uszkoreit∗ Google Research usz@google.com Llion Jones∗ Google Research llion@google.comAidan N. Gomez∗† University of Toronto aidan@cs.toronto.eduŁukasz Kaiser∗ Google Brain lukaszkaiser@google.com Illia Polosukhin∗‡ illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":19}],"source":["rag_tool.run(\"How did self-attention mechanism evolve in large language models?\")"]},{"cell_type":"markdown","id":"0ac98c3c-b02f-4680-8fab-99fbfa3c45a3","metadata":{"language":"python","id":"0ac98c3c-b02f-4680-8fab-99fbfa3c45a3"},"source":["## Mention the Tavily API Key"]},{"cell_type":"code","execution_count":20,"id":"4e1b4e5e-a4c4-4b81-8d11-97b9f7eb1d4f","metadata":{"execution":{"iopub.execute_input":"2024-08-06T09:10:33.516323Z","iopub.status.busy":"2024-08-06T09:10:33.515939Z","iopub.status.idle":"2024-08-06T09:10:33.520768Z","shell.execute_reply":"2024-08-06T09:10:33.520117Z","shell.execute_reply.started":"2024-08-06T09:10:33.516295Z"},"language":"python","trusted":true,"id":"4e1b4e5e-a4c4-4b81-8d11-97b9f7eb1d4f","executionInfo":{"status":"ok","timestamp":1736957308604,"user_tz":480,"elapsed":729,"user":{"displayName":"Bruce Kyle","userId":"12761463034575353107"}}},"outputs":[],"source":["import os\n","from google.colab import userdata\n","TAVILY_API_KEY = userdata.get('TAVILY_API_KEY')\n","\n","# Set the Tavily API key\n","os.environ['TAVILY_API_KEY'] = TAVILY_API_KEY"]},{"cell_type":"code","execution_count":21,"id":"086f3f46-0750-4d59-a56a-02d13b802223","metadata":{"execution":{"iopub.execute_input":"2024-08-06T09:10:40.163940Z","iopub.status.busy":"2024-08-06T09:10:40.163524Z","iopub.status.idle":"2024-08-06T09:10:40.167780Z","shell.execute_reply":"2024-08-06T09:10:40.167028Z","shell.execute_reply.started":"2024-08-06T09:10:40.163904Z"},"language":"python","trusted":true,"id":"086f3f46-0750-4d59-a56a-02d13b802223","executionInfo":{"status":"ok","timestamp":1736957308604,"user_tz":480,"elapsed":4,"user":{"displayName":"Bruce Kyle","userId":"12761463034575353107"}}},"outputs":[],"source":["web_search_tool = TavilySearchResults(k=3)"]},{"cell_type":"code","execution_count":22,"id":"b445a6f5-ff84-4ee8-b633-f28be3d06b00","metadata":{"execution":{"iopub.execute_input":"2024-08-06T09:10:41.783474Z","iopub.status.busy":"2024-08-06T09:10:41.782945Z","iopub.status.idle":"2024-08-06T09:10:43.410666Z","shell.execute_reply":"2024-08-06T09:10:43.409742Z","shell.execute_reply.started":"2024-08-06T09:10:41.783445Z"},"language":"python","trusted":true,"id":"b445a6f5-ff84-4ee8-b633-f28be3d06b00","outputId":"ac856412-794f-4688-8cf3-d2c4d84b3aa6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736957310590,"user_tz":480,"elapsed":1990,"user":{"displayName":"Bruce Kyle","userId":"12761463034575353107"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'url': 'https://glasp.co/hatch/7hz3t43jrtii5if6/p/pz1mQPOVZVdSiHasrCvK',\n","  'content': 'The self-attention mechanism is a fundamental building block of large language models like Transformers. It enables the model to capture the dependencies between different words in a sentence. The mechanism assigns weights to each word in the input sequence based on its relevance to other words.'},\n"," {'url': 'https://medium.com/data-science-community-srm/exploring-attention-mechanisms-in-large-language-models-33549ae2c5b8',\n","  'content': \"These models, think of them as super-smart text processors, use something called 'self-attention.' Now, self-attention is a feature in their design that helps them understand words and\"},\n"," {'url': 'https://incubity.ambilio.com/self-attention-mechanism-in-transformer-based-llms/',\n","  'content': \"Role of Self-Attention in Large Language Models. Self-attention plays a crucial role in empowering Transformer-based large language models to achieve remarkable performance in NLP tasks. Here's how: Capturing Long-Range Dependencies: One of the key strengths of self-attention is its ability to capture long-range dependencies within input\"},\n"," {'url': 'https://lightning.ai/courses/deep-learning-fundamentals/unit-8.0-natural-language-processing-and-large-language-models/8.5-understanding-self-attention/',\n","  'content': 'References. Attention Is All You Need (2018) — the original transformer paper; What we covered in this video lecture. To understand large language transformers, it is essential to understand self-attention, which is the underlying mechanism that powers these models: self-attention can be understood as a way to create context-aware text embedding vectors.'},\n"," {'url': 'https://phaneendrakn.medium.com/a-primer-on-understanding-attention-mechanisms-in-llms-fda4051b00a1',\n","  'content': 'Attention mechanisms have fundamentally transformed the capabilities of Large Language Models (LLMs), enabling them to effectively process and generate natural language. By refining these mechanisms, we empower machines to engage with human language more comprehensively, introducing a new era of communication and understanding in the digital'}]"]},"metadata":{},"execution_count":22}],"source":["web_search_tool.run(\"What is self-attention mechansim in large language models?\")"]},{"cell_type":"markdown","id":"d1e69851-f7e3-4b9a-8c7c-7da0f7f04bde","metadata":{"language":"python","id":"d1e69851-f7e3-4b9a-8c7c-7da0f7f04bde"},"source":["## Define a Tool"]},{"cell_type":"code","execution_count":23,"id":"907b371c-4b24-4c80-b833-971f56ad4c1f","metadata":{"execution":{"iopub.execute_input":"2024-08-06T09:11:34.946830Z","iopub.status.busy":"2024-08-06T09:11:34.946268Z","iopub.status.idle":"2024-08-06T09:11:34.952558Z","shell.execute_reply":"2024-08-06T09:11:34.951976Z","shell.execute_reply.started":"2024-08-06T09:11:34.946789Z"},"language":"python","trusted":true,"id":"907b371c-4b24-4c80-b833-971f56ad4c1f","executionInfo":{"status":"ok","timestamp":1736957310590,"user_tz":480,"elapsed":4,"user":{"displayName":"Bruce Kyle","userId":"12761463034575353107"}}},"outputs":[],"source":["@tool\n","def router_tool(question):\n","  \"\"\"Router Function\"\"\"\n","  if 'self-attention' in question:\n","    return 'vectorstore'\n","  else:\n","    return 'web_search'"]},{"cell_type":"markdown","id":"be17dc59-0e72-4eaa-9654-400df6cb371e","metadata":{"language":"python","id":"be17dc59-0e72-4eaa-9654-400df6cb371e"},"source":["## Create Agents to work with"]},{"cell_type":"code","execution_count":24,"id":"6cacf6a3-7f50-49d6-9ab0-b63433d7032c","metadata":{"execution":{"iopub.execute_input":"2024-08-06T09:11:39.241558Z","iopub.status.busy":"2024-08-06T09:11:39.241051Z","iopub.status.idle":"2024-08-06T09:11:39.249728Z","shell.execute_reply":"2024-08-06T09:11:39.248634Z","shell.execute_reply.started":"2024-08-06T09:11:39.241517Z"},"language":"python","trusted":true,"id":"6cacf6a3-7f50-49d6-9ab0-b63433d7032c","executionInfo":{"status":"ok","timestamp":1736957310590,"user_tz":480,"elapsed":4,"user":{"displayName":"Bruce Kyle","userId":"12761463034575353107"}}},"outputs":[],"source":["Router_Agent = Agent(\n","  role='Router',\n","  goal='Route user question to a vectorstore or web search',\n","  backstory=(\n","    \"You are an expert at routing a user question to a vectorstore or web search.\"\n","    \"Use the vectorstore for questions on concept related to Retrieval-Augmented Generation.\"\n","    \"You do not need to be stringent with the keywords in the question related to these topics. Otherwise, use web-search.\"\n","  ),\n","  verbose=True,\n","  allow_delegation=False,\n","  llm=llm,\n",")"]},{"cell_type":"code","execution_count":25,"id":"4fc04747-af23-4b19-9ece-6473c683f739","metadata":{"execution":{"iopub.execute_input":"2024-08-06T09:11:41.155843Z","iopub.status.busy":"2024-08-06T09:11:41.155481Z","iopub.status.idle":"2024-08-06T09:11:41.162015Z","shell.execute_reply":"2024-08-06T09:11:41.161341Z","shell.execute_reply.started":"2024-08-06T09:11:41.155815Z"},"language":"python","trusted":true,"id":"4fc04747-af23-4b19-9ece-6473c683f739","executionInfo":{"status":"ok","timestamp":1736957310590,"user_tz":480,"elapsed":4,"user":{"displayName":"Bruce Kyle","userId":"12761463034575353107"}}},"outputs":[],"source":["Retriever_Agent = Agent(\n","role=\"Retriever\",\n","goal=\"Use the information retrieved from the vectorstore to answer the question\",\n","backstory=(\n","    \"You are an assistant for question-answering tasks.\"\n","    \"Use the information present in the retrieved context to answer the question.\"\n","    \"You have to provide a clear concise answer.\"\n","),\n","verbose=True,\n","allow_delegation=False,\n","llm=llm,\n",")"]},{"cell_type":"code","execution_count":26,"id":"25ece7eb-37fa-450d-991b-16dd8abca495","metadata":{"execution":{"iopub.execute_input":"2024-08-06T09:11:42.476081Z","iopub.status.busy":"2024-08-06T09:11:42.475654Z","iopub.status.idle":"2024-08-06T09:11:42.489959Z","shell.execute_reply":"2024-08-06T09:11:42.485582Z","shell.execute_reply.started":"2024-08-06T09:11:42.476042Z"},"language":"python","trusted":true,"id":"25ece7eb-37fa-450d-991b-16dd8abca495","executionInfo":{"status":"ok","timestamp":1736957310590,"user_tz":480,"elapsed":4,"user":{"displayName":"Bruce Kyle","userId":"12761463034575353107"}}},"outputs":[],"source":["Grader_agent =  Agent(\n","  role='Answer Grader',\n","  goal='Filter out erroneous retrievals',\n","  backstory=(\n","    \"You are a grader assessing relevance of a retrieved document to a user question.\"\n","    \"If the document contains keywords related to the user question, grade it as relevant.\"\n","    \"It does not need to be a stringent test.You have to make sure that the answer is relevant to the question.\"\n","  ),\n","  verbose=True,\n","  allow_delegation=False,\n","  llm=llm,\n",")"]},{"cell_type":"code","execution_count":27,"id":"5cdb03f9-34bc-400b-8ada-80c08a71f1f8","metadata":{"execution":{"iopub.execute_input":"2024-08-06T09:11:43.725087Z","iopub.status.busy":"2024-08-06T09:11:43.724709Z","iopub.status.idle":"2024-08-06T09:11:43.731283Z","shell.execute_reply":"2024-08-06T09:11:43.730399Z","shell.execute_reply.started":"2024-08-06T09:11:43.725058Z"},"language":"python","trusted":true,"id":"5cdb03f9-34bc-400b-8ada-80c08a71f1f8","executionInfo":{"status":"ok","timestamp":1736957310590,"user_tz":480,"elapsed":3,"user":{"displayName":"Bruce Kyle","userId":"12761463034575353107"}}},"outputs":[],"source":["hallucination_grader = Agent(\n","    role=\"Hallucination Grader\",\n","    goal=\"Filter out hallucination\",\n","    backstory=(\n","        \"You are a hallucination grader assessing whether an answer is grounded in / supported by a set of facts.\"\n","        \"Make sure you meticulously review the answer and check if the response provided is in alignmnet with the question asked\"\n","    ),\n","    verbose=True,\n","    allow_delegation=False,\n","    llm=llm,\n",")"]},{"cell_type":"code","execution_count":28,"id":"b6e3e019-18c4-4f33-be69-628274a544e2","metadata":{"execution":{"iopub.execute_input":"2024-08-06T09:11:48.359638Z","iopub.status.busy":"2024-08-06T09:11:48.359179Z","iopub.status.idle":"2024-08-06T09:11:48.372283Z","shell.execute_reply":"2024-08-06T09:11:48.369019Z","shell.execute_reply.started":"2024-08-06T09:11:48.359547Z"},"language":"python","trusted":true,"id":"b6e3e019-18c4-4f33-be69-628274a544e2","executionInfo":{"status":"ok","timestamp":1736957310590,"user_tz":480,"elapsed":3,"user":{"displayName":"Bruce Kyle","userId":"12761463034575353107"}}},"outputs":[],"source":["answer_grader = Agent(\n","    role=\"Answer Grader\",\n","    goal=\"Filter out hallucination from the answer.\",\n","    backstory=(\n","        \"You are a grader assessing whether an answer is useful to resolve a question.\"\n","        \"Make sure you meticulously review the answer and check if it makes sense for the question asked\"\n","        \"If the answer is relevant generate a clear and concise response.\"\n","        \"If the answer gnerated is not relevant then perform a websearch using 'web_search_tool'\"\n","    ),\n","    verbose=True,\n","    allow_delegation=False,\n","    llm=llm,\n",")"]},{"cell_type":"markdown","id":"626670c9-717e-4bcd-b0b4-e4d9aaaae7eb","metadata":{"language":"python","id":"626670c9-717e-4bcd-b0b4-e4d9aaaae7eb"},"source":["## Defining Tasks"]},{"cell_type":"code","execution_count":29,"id":"5e9c9242-5b00-4e59-8e3d-d904c8e9d21f","metadata":{"execution":{"iopub.execute_input":"2024-08-06T09:11:50.881204Z","iopub.status.busy":"2024-08-06T09:11:50.880708Z","iopub.status.idle":"2024-08-06T09:11:50.886480Z","shell.execute_reply":"2024-08-06T09:11:50.885321Z","shell.execute_reply.started":"2024-08-06T09:11:50.881168Z"},"language":"python","trusted":true,"id":"5e9c9242-5b00-4e59-8e3d-d904c8e9d21f","executionInfo":{"status":"ok","timestamp":1736957310803,"user_tz":480,"elapsed":216,"user":{"displayName":"Bruce Kyle","userId":"12761463034575353107"}}},"outputs":[],"source":["router_task = Task(\n","    description=(\"Analyse the keywords in the question {question}\"\n","    \"Based on the keywords decide whether it is eligible for a vectorstore search or a web search.\"\n","    \"Return a single word 'vectorstore' if it is eligible for vectorstore search.\"\n","    \"Return a single word 'websearch' if it is eligible for web search.\"\n","    \"Do not provide any other premable or explaination.\"\n","    ),\n","    expected_output=(\"Give a binary choice 'websearch' or 'vectorstore' based on the question\"\n","    \"Do not provide any other premable or explaination.\"),\n","    agent=Router_Agent,\n","    tools=[router_tool],\n",")"]},{"cell_type":"code","execution_count":30,"id":"5a497afe-0fd0-4b9f-a97b-64d1b5518ef0","metadata":{"execution":{"iopub.execute_input":"2024-08-06T09:11:52.682162Z","iopub.status.busy":"2024-08-06T09:11:52.681767Z","iopub.status.idle":"2024-08-06T09:11:52.689821Z","shell.execute_reply":"2024-08-06T09:11:52.689037Z","shell.execute_reply.started":"2024-08-06T09:11:52.682135Z"},"language":"python","trusted":true,"id":"5a497afe-0fd0-4b9f-a97b-64d1b5518ef0","executionInfo":{"status":"ok","timestamp":1736957310803,"user_tz":480,"elapsed":2,"user":{"displayName":"Bruce Kyle","userId":"12761463034575353107"}}},"outputs":[],"source":["retriever_task = Task(\n","    description=(\"Based on the response from the router task extract information for the question {question} with the help of the respective tool.\"\n","    \"Use the web_serach_tool to retrieve information from the web in case the router task output is 'websearch'.\"\n","    \"Use the rag_tool to retrieve information from the vectorstore in case the router task output is 'vectorstore'.\"\n","    ),\n","    expected_output=(\"You should analyse the output of the 'router_task'\"\n","    \"If the response is 'websearch' then use the web_search_tool to retrieve information from the web.\"\n","    \"If the response is 'vectorstore' then use the rag_tool to retrieve information from the vectorstore.\"\n","    \"Return a claer and consise text as response.\"),\n","    agent=Retriever_Agent,\n","    context=[router_task],\n","   #tools=[retriever_tool],\n",")"]},{"cell_type":"code","execution_count":31,"id":"ce51750e-941d-41fa-b4d4-d36f7e56961e","metadata":{"execution":{"iopub.execute_input":"2024-08-06T09:11:54.826584Z","iopub.status.busy":"2024-08-06T09:11:54.825901Z","iopub.status.idle":"2024-08-06T09:11:54.831307Z","shell.execute_reply":"2024-08-06T09:11:54.830655Z","shell.execute_reply.started":"2024-08-06T09:11:54.826545Z"},"language":"python","trusted":true,"id":"ce51750e-941d-41fa-b4d4-d36f7e56961e","executionInfo":{"status":"ok","timestamp":1736957310804,"user_tz":480,"elapsed":3,"user":{"displayName":"Bruce Kyle","userId":"12761463034575353107"}}},"outputs":[],"source":["grader_task = Task(\n","    description=(\"Based on the response from the retriever task for the quetion {question} evaluate whether the retrieved content is relevant to the question.\"\n","    ),\n","    expected_output=(\"Binary score 'yes' or 'no' score to indicate whether the document is relevant to the question\"\n","    \"You must answer 'yes' if the response from the 'retriever_task' is in alignment with the question asked.\"\n","    \"You must answer 'no' if the response from the 'retriever_task' is not in alignment with the question asked.\"\n","    \"Do not provide any preamble or explanations except for 'yes' or 'no'.\"),\n","    agent=Grader_agent,\n","    context=[retriever_task],\n",")"]},{"cell_type":"code","execution_count":32,"id":"0d1f89f1-caef-43ea-9ec7-614467f73a3d","metadata":{"execution":{"iopub.execute_input":"2024-08-06T09:11:58.436290Z","iopub.status.busy":"2024-08-06T09:11:58.435873Z","iopub.status.idle":"2024-08-06T09:11:58.443357Z","shell.execute_reply":"2024-08-06T09:11:58.442433Z","shell.execute_reply.started":"2024-08-06T09:11:58.436248Z"},"language":"python","trusted":true,"id":"0d1f89f1-caef-43ea-9ec7-614467f73a3d","executionInfo":{"status":"ok","timestamp":1736957310804,"user_tz":480,"elapsed":3,"user":{"displayName":"Bruce Kyle","userId":"12761463034575353107"}}},"outputs":[],"source":["hallucination_task = Task(\n","    description=(\"Based on the response from the grader task for the quetion {question} evaluate whether the answer is grounded in / supported by a set of facts.\"),\n","    expected_output=(\"Binary score 'yes' or 'no' score to indicate whether the answer is sync with the question asked\"\n","    \"Respond 'yes' if the answer is in useful and contains fact about the question asked.\"\n","    \"Respond 'no' if the answer is not useful and does not contains fact about the question asked.\"\n","    \"Do not provide any preamble or explanations except for 'yes' or 'no'.\"),\n","    agent=hallucination_grader,\n","    context=[grader_task],\n",")\n","\n","answer_task = Task(\n","    description=(\"Based on the response from the hallucination task for the quetion {question} evaluate whether the answer is useful to resolve the question.\"\n","    \"If the answer is 'yes' return a clear and concise answer.\"\n","    \"If the answer is 'no' then perform a 'websearch' and return the response\"),\n","    expected_output=(\"Return a clear and concise response if the response from 'hallucination_task' is 'yes'.\"\n","    \"Perform a web search using 'web_search_tool' and return ta clear and concise response only if the response from 'hallucination_task' is 'no'.\"\n","    \"Otherwise respond as 'Sorry! unable to find a valid response'.\"),\n","    context=[hallucination_task],\n","    agent=answer_grader,\n","    #tools=[answer_grader_tool],\n",")"]},{"cell_type":"markdown","id":"190bfcbf-66bd-4ddc-bde6-2a96a002b8d7","metadata":{"language":"python","id":"190bfcbf-66bd-4ddc-bde6-2a96a002b8d7"},"source":["## Define a flow for the use case"]},{"cell_type":"code","execution_count":33,"id":"75f9cbeb-d342-4da9-b442-b13e66ebbc96","metadata":{"execution":{"iopub.execute_input":"2024-08-06T09:12:01.597715Z","iopub.status.busy":"2024-08-06T09:12:01.597364Z","iopub.status.idle":"2024-08-06T09:12:01.609224Z","shell.execute_reply":"2024-08-06T09:12:01.608693Z","shell.execute_reply.started":"2024-08-06T09:12:01.597687Z"},"language":"python","trusted":true,"id":"75f9cbeb-d342-4da9-b442-b13e66ebbc96","executionInfo":{"status":"ok","timestamp":1736957310804,"user_tz":480,"elapsed":2,"user":{"displayName":"Bruce Kyle","userId":"12761463034575353107"}}},"outputs":[],"source":["rag_crew = Crew(\n","    agents=[Router_Agent, Retriever_Agent, Grader_agent, hallucination_grader, answer_grader],\n","    tasks=[router_task, retriever_task, grader_task, hallucination_task, answer_task],\n","    verbose=True,\n","\n",")"]},{"cell_type":"markdown","id":"957af34a-ffc4-4283-9e26-3c936f8719cc","metadata":{"language":"python","id":"957af34a-ffc4-4283-9e26-3c936f8719cc"},"source":["## Ask any query"]},{"cell_type":"code","execution_count":35,"id":"b98e365a-cd84-4664-941e-5fb40765d7dc","metadata":{"execution":{"iopub.execute_input":"2024-08-06T09:12:12.076469Z","iopub.status.busy":"2024-08-06T09:12:12.075913Z","iopub.status.idle":"2024-08-06T09:12:12.089371Z","shell.execute_reply":"2024-08-06T09:12:12.082684Z","shell.execute_reply.started":"2024-08-06T09:12:12.076435Z"},"language":"python","trusted":true,"id":"b98e365a-cd84-4664-941e-5fb40765d7dc","executionInfo":{"status":"ok","timestamp":1736957330861,"user_tz":480,"elapsed":131,"user":{"displayName":"Bruce Kyle","userId":"12761463034575353107"}}},"outputs":[],"source":["inputs ={\"question\":\"How does self-attention mechanism help large language models?\"}"]},{"cell_type":"markdown","id":"c3ebcad1-ab52-4ed9-a065-485cac2a4362","metadata":{"language":"python","id":"c3ebcad1-ab52-4ed9-a065-485cac2a4362"},"source":["## Kick off the agent pipeline"]},{"cell_type":"code","execution_count":36,"id":"4fcd4ac6-a176-425f-8bf9-a15ef0075381","metadata":{"execution":{"iopub.execute_input":"2024-08-06T09:12:16.098251Z","iopub.status.busy":"2024-08-06T09:12:16.097908Z","iopub.status.idle":"2024-08-06T09:12:22.448360Z","shell.execute_reply":"2024-08-06T09:12:22.447696Z","shell.execute_reply.started":"2024-08-06T09:12:16.098211Z"},"language":"python","trusted":true,"id":"4fcd4ac6-a176-425f-8bf9-a15ef0075381","outputId":"7190b617-1665-4989-dc98-436d92e03294","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736957369796,"user_tz":480,"elapsed":36452,"user":{"displayName":"Bruce Kyle","userId":"12761463034575353107"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m\u001b[95m [DEBUG]: == Working Agent: Router\u001b[00m\n","\u001b[1m\u001b[95m [INFO]: == Starting Task: Analyse the keywords in the question How does self-attention mechanism help large language models?Based on the keywords decide whether it is eligible for a vectorstore search or a web search.Return a single word 'vectorstore' if it is eligible for vectorstore search.Return a single word 'websearch' if it is eligible for web search.Do not provide any other premable or explaination.\u001b[00m\n","\n","\n","\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3mThought: The question seems to be related to the concept of self-attention mechanism in large language models, which is a topic in Retrieval-Augmented Generation.\n","Action: router_tool\n","Action Input: {\"question\": \"How does self-attention mechanism help large language models?\"}\u001b[0m\u001b[95m \n","\n","vectorstore\n","\u001b[00m\n","\u001b[32;1m\u001b[1;3mFinal Answer: vectorstore\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\u001b[1m\u001b[92m [DEBUG]: == [Router] Task output: vectorstore\n","\n","\u001b[00m\n","\u001b[1m\u001b[95m [DEBUG]: == Working Agent: Retriever\u001b[00m\n","\u001b[1m\u001b[95m [INFO]: == Starting Task: Based on the response from the router task extract information for the question How does self-attention mechanism help large language models? with the help of the respective tool.Use the web_serach_tool to retrieve information from the web in case the router task output is 'websearch'.Use the rag_tool to retrieve information from the vectorstore in case the router task output is 'vectorstore'.\u001b[00m\n","\n","\n","\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3mThought: I now can give a great answer\n","\n","Action: I will analyze the output of the 'router_task' and provide a clear and concise answer based on the retrieved information from the vectorstore.\n","\n","Final Answer: The self-attention mechanism in large language models helps in several ways. Firstly, it allows the model to focus on specific parts of the input sequence that are relevant to the task at hand. This is particularly useful for tasks that require understanding complex relationships between different parts of the input, such as sentiment analysis or question answering. Secondly, self-attention enables the model to capture long-range dependencies in the input sequence, which is important for tasks that require understanding the context in which a piece of text is used. Finally, self-attention allows the model to weigh the importance of different parts of the input sequence based on their relevance to the task, which can improve the overall accuracy of the model.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\u001b[1m\u001b[92m [DEBUG]: == [Retriever] Task output: The self-attention mechanism in large language models helps in several ways. Firstly, it allows the model to focus on specific parts of the input sequence that are relevant to the task at hand. This is particularly useful for tasks that require understanding complex relationships between different parts of the input, such as sentiment analysis or question answering. Secondly, self-attention enables the model to capture long-range dependencies in the input sequence, which is important for tasks that require understanding the context in which a piece of text is used. Finally, self-attention allows the model to weigh the importance of different parts of the input sequence based on their relevance to the task, which can improve the overall accuracy of the model.\n","\n","\u001b[00m\n","\u001b[1m\u001b[95m [DEBUG]: == Working Agent: Answer Grader\u001b[00m\n","\u001b[1m\u001b[95m [INFO]: == Starting Task: Based on the response from the retriever task for the quetion How does self-attention mechanism help large language models? evaluate whether the retrieved content is relevant to the question.\u001b[00m\n","\n","\n","\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3mThought: I now can give a great answer\n","Final Answer: yes\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\u001b[1m\u001b[92m [DEBUG]: == [Answer Grader] Task output: yes\n","\n","\u001b[00m\n","\u001b[1m\u001b[95m [DEBUG]: == Working Agent: Hallucination Grader\u001b[00m\n","\u001b[1m\u001b[95m [INFO]: == Starting Task: Based on the response from the grader task for the quetion How does self-attention mechanism help large language models? evaluate whether the answer is grounded in / supported by a set of facts.\u001b[00m\n","\n","\n","\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3mThought: I will meticulously review the answer and check if the response provided is in alignment with the question asked. I will evaluate whether the answer is grounded in/supported by a set of facts.\n","\n","Final Answer: yes\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\u001b[1m\u001b[92m [DEBUG]: == [Hallucination Grader] Task output: yes\n","\n","\u001b[00m\n","\u001b[1m\u001b[95m [DEBUG]: == Working Agent: Answer Grader\u001b[00m\n","\u001b[1m\u001b[95m [INFO]: == Starting Task: Based on the response from the hallucination task for the quetion How does self-attention mechanism help large language models? evaluate whether the answer is useful to resolve the question.If the answer is 'yes' return a clear and concise answer.If the answer is 'no' then perform a 'websearch' and return the response\u001b[00m\n","\n","\n","\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3mThought: I now can give a great answer\n","\n","Action: Since the response from the hallucination task is \"yes\", I will provide a clear and concise answer.\n","\n","Final Answer: The self-attention mechanism in large language models helps by allowing the model to focus on specific parts of the input sequence that are relevant to the task at hand. This is in contrast to traditional recurrent neural networks (RNNs) that process the input sequence sequentially and may not be able to capture long-range dependencies. Self-attention enables the model to attend to different parts of the input sequence simultaneously and weigh their importance, which can lead to better performance on tasks such as machine translation, text summarization, and question answering.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\u001b[1m\u001b[92m [DEBUG]: == [Answer Grader] Task output: The self-attention mechanism in large language models helps by allowing the model to focus on specific parts of the input sequence that are relevant to the task at hand. This is in contrast to traditional recurrent neural networks (RNNs) that process the input sequence sequentially and may not be able to capture long-range dependencies. Self-attention enables the model to attend to different parts of the input sequence simultaneously and weigh their importance, which can lead to better performance on tasks such as machine translation, text summarization, and question answering.\n","\n","\u001b[00m\n"]}],"source":["result = rag_crew.kickoff(inputs=inputs)"]},{"cell_type":"markdown","id":"caf9e58d-3c00-4cea-83d9-806a016f740b","metadata":{"language":"python","id":"caf9e58d-3c00-4cea-83d9-806a016f740b"},"source":["## Obtain the final response"]},{"cell_type":"code","execution_count":37,"id":"49e437fa-c3b3-4d1e-983b-93713a2df989","metadata":{"execution":{"iopub.execute_input":"2024-08-06T09:12:44.362032Z","iopub.status.busy":"2024-08-06T09:12:44.361665Z","iopub.status.idle":"2024-08-06T09:12:44.366198Z","shell.execute_reply":"2024-08-06T09:12:44.365344Z","shell.execute_reply.started":"2024-08-06T09:12:44.362005Z"},"language":"python","trusted":true,"id":"49e437fa-c3b3-4d1e-983b-93713a2df989","outputId":"d6f35fd4-a122-4ffc-8093-957db8bad23e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736957410040,"user_tz":480,"elapsed":194,"user":{"displayName":"Bruce Kyle","userId":"12761463034575353107"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["The self-attention mechanism in large language models helps by allowing the model to focus on specific parts of the input sequence that are relevant to the task at hand. This is in contrast to traditional recurrent neural networks (RNNs) that process the input sequence sequentially and may not be able to capture long-range dependencies. Self-attention enables the model to attend to different parts of the input sequence simultaneously and weigh their importance, which can lead to better performance on tasks such as machine translation, text summarization, and question answering.\n"]}],"source":["print(result)"]}],"metadata":{"jupyterlab":{"notebooks":{"version_major":6,"version_minor":4}},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"},"singlestore_cell_default_language":"python","singlestore_connection":{"connectionID":"a22b6f8b-b11b-4979-98da-98513e9876e6","defaultDatabase":""},"singlestore_row_limit":300,"colab":{"provenance":[{"file_id":"1oLdlwjWYah06JhBY1S8uHqxGwjIHHDck","timestamp":1736891557493}]}},"nbformat":4,"nbformat_minor":5}